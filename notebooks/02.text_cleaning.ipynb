{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19118ccf",
   "metadata": {},
   "source": [
    "# Text Cleaning Pipeline for Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive text cleaning pipeline for Twitter sentiment analysis data. The goal is to preprocess raw tweets to make them suitable for various word embedding techniques and model architectures.\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: Twitter_Data.csv\n",
    "- **Columns**: clean_text (tweet content), category (sentiment label: -1, 0, 1)\n",
    "- **Sentiment Labels**: \n",
    "  - -1: Negative\n",
    "  - 0: Neutral\n",
    "  - 1: Positive\n",
    "\n",
    "## Text Cleaning Approach\n",
    "We will implement multiple cleaning strategies:\n",
    "1. **Basic Cleaning**: Remove URLs, mentions, hashtags, special characters\n",
    "2. **Normalization**: Lowercase conversion, whitespace normalization\n",
    "3. **Noise Removal**: Remove numbers, punctuation, extra spaces\n",
    "4. **Advanced Cleaning**: Handle contractions, remove stopwords, lemmatization\n",
    "5. **Multiple Versions**: Create different cleaned versions for different embedding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6c49c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5caa8",
   "metadata": {},
   "source": [
    "## 2. Download NLTK Resources\n",
    "\n",
    "Download necessary NLTK data for text processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk_resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4', 'punkt_tab']\n",
    "\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "        print(f\"âœ“ {resource} downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error downloading {resource}: {e}\")\n",
    "\n",
    "print(\"\\nNLTK resources ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36d9f4",
   "metadata": {},
   "source": [
    "## 3. Load Raw Data\n",
    "\n",
    "Load the Twitter sentiment dataset from the raw data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/raw/Twitter_Data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nSentiment Distribution:\\n{df['category'].value_counts().sort_index()}\")\n",
    "\n",
    "# Display sample records\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Records:\")\n",
    "print(\"=\"*80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175eac7b",
   "metadata": {},
   "source": [
    "## 4. Exploratory Analysis of Raw Text\n",
    "\n",
    "Analyze the characteristics of raw text data before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d003b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text length distribution\n",
    "df['text_length'] = df['clean_text'].astype(str).apply(len)\n",
    "df['word_count'] = df['clean_text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())\n",
    "\n",
    "# Check for special patterns\n",
    "df['has_url'] = df['clean_text'].astype(str).apply(lambda x: bool(re.search(r'http[s]?://\\S+|www\\.\\S+', x)))\n",
    "df['has_mention'] = df['clean_text'].astype(str).apply(lambda x: bool(re.search(r'@\\w+', x)))\n",
    "df['has_hashtag'] = df['clean_text'].astype(str).apply(lambda x: bool(re.search(r'#\\w+', x)))\n",
    "df['has_number'] = df['clean_text'].astype(str).apply(lambda x: bool(re.search(r'\\d+', x)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Special Pattern Presence:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Tweets with URLs: {df['has_url'].sum()} ({df['has_url'].mean()*100:.2f}%)\")\n",
    "print(f\"Tweets with Mentions: {df['has_mention'].sum()} ({df['has_mention'].mean()*100:.2f}%)\")\n",
    "print(f\"Tweets with Hashtags: {df['has_hashtag'].sum()} ({df['has_hashtag'].mean()*100:.2f}%)\")\n",
    "print(f\"Tweets with Numbers: {df['has_number'].sum()} ({df['has_number'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution by sentiment\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Word count distribution\n",
    "axes[0].hist([df[df['category']==-1]['word_count'], \n",
    "              df[df['category']==0]['word_count'], \n",
    "              df[df['category']==1]['word_count']], \n",
    "             label=['Negative', 'Neutral', 'Positive'], \n",
    "             bins=30, alpha=0.7)\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Word Count Distribution by Sentiment')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Character length distribution\n",
    "axes[1].hist([df[df['category']==-1]['text_length'], \n",
    "              df[df['category']==0]['text_length'], \n",
    "              df[df['category']==1]['text_length']], \n",
    "             label=['Negative', 'Neutral', 'Positive'], \n",
    "             bins=30, alpha=0.7)\n",
    "axes[1].set_xlabel('Character Length')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Character Length Distribution by Sentiment')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c057b",
   "metadata": {},
   "source": [
    "## 5. Define Text Cleaning Functions\n",
    "\n",
    "Create modular functions for different text cleaning operations. Each function serves a specific purpose in the cleaning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df60902",
   "metadata": {},
   "source": [
    "### 5.1 Basic Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove URLs from text.\n",
    "    Handles http://, https://, www., and shortened URLs.\n",
    "    \"\"\"\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    text = re.sub(r'www\\.\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\"\n",
    "    Remove Twitter mentions (@username).\n",
    "    \"\"\"\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"\n",
    "    Remove hashtag symbols but keep the text.\n",
    "    Example: #modi -> modi\n",
    "    \"\"\"\n",
    "    return re.sub(r'#', '', text)\n",
    "\n",
    "def remove_special_characters(text, keep_punctuation=False):\n",
    "    \"\"\"\n",
    "    Remove special characters and digits.\n",
    "    Option to keep basic punctuation for some embedding techniques.\n",
    "    \"\"\"\n",
    "    if keep_punctuation:\n",
    "        # Keep basic punctuation: . , ! ?\n",
    "        pattern = r'[^a-zA-Z\\s.,!?]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"\n",
    "    Remove extra whitespace, tabs, and newlines.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"\n",
    "    Convert text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "print(\"âœ“ Basic cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6a14e",
   "metadata": {},
   "source": [
    "### 5.2 Advanced Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa22c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contractions dictionary\n",
    "CONTRACTIONS = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"'s\": \"\",  # Handle possessives\n",
    "    \"'re\": \" are\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Expand contractions in text.\n",
    "    Example: \"don't\" -> \"do not\"\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    for contraction, expansion in CONTRACTIONS.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Remove stopwords from text.\n",
    "    Option to add custom stopwords for domain-specific filtering.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if custom_stopwords:\n",
    "        stop_words.update(custom_stopwords)\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize words in text.\n",
    "    Reduces words to their base/dictionary form.\n",
    "    Example: \"running\" -> \"run\", \"better\" -> \"good\"\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Apply Porter Stemming to text.\n",
    "    More aggressive than lemmatization.\n",
    "    Example: \"running\" -> \"run\", \"happiness\" -> \"happi\"\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def remove_short_words(text, min_length=2):\n",
    "    \"\"\"\n",
    "    Remove words shorter than specified length.\n",
    "    Helps remove noise like single characters.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if len(word) >= min_length]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    \"\"\"\n",
    "    Reduce repeated characters to max 2.\n",
    "    Example: \"goooood\" -> \"good\", \"happyyy\" -> \"happyy\"\n",
    "    \"\"\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "print(\"âœ“ Advanced cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3e004",
   "metadata": {},
   "source": [
    "### 5.3 Combined Cleaning Pipelines\n",
    "\n",
    "Create different cleaning pipelines optimized for various word embedding and model architectures:\n",
    "\n",
    "1. **Basic Clean**: For traditional embeddings (Word2Vec, GloVe)\n",
    "2. **Full Clean with Lemmatization**: For TF-IDF and classical ML models\n",
    "3. **Minimal Clean**: For transformer-based models (BERT, RoBERTa) that handle raw text better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_basic(text):\n",
    "    \"\"\"\n",
    "    Basic cleaning pipeline - suitable for Word2Vec, GloVe, FastText\n",
    "    \n",
    "    Steps:\n",
    "    1. Handle NaN/None values\n",
    "    2. Remove URLs\n",
    "    3. Remove mentions\n",
    "    4. Remove hashtag symbols\n",
    "    5. Convert to lowercase\n",
    "    6. Remove special characters (keep only letters)\n",
    "    7. Remove extra whitespace\n",
    "    8. Remove short words (length < 2)\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    text = remove_special_characters(text, keep_punctuation=False)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    text = remove_short_words(text, min_length=2)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text_full(text):\n",
    "    \"\"\"\n",
    "    Full cleaning pipeline - suitable for TF-IDF, Bag of Words, Classical ML\n",
    "    \n",
    "    Steps:\n",
    "    1. All basic cleaning steps\n",
    "    2. Expand contractions\n",
    "    3. Remove stopwords\n",
    "    4. Lemmatization\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    text = remove_special_characters(text, keep_punctuation=False)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_short_words(text, min_length=2)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text_minimal(text):\n",
    "    \"\"\"\n",
    "    Minimal cleaning pipeline - suitable for BERT, RoBERTa, Transformers\n",
    "    \n",
    "    Transformers work better with more natural text, so we apply minimal cleaning:\n",
    "    1. Handle NaN/None values\n",
    "    2. Remove URLs\n",
    "    3. Remove mentions (optional, keeps @)\n",
    "    4. Remove extra whitespace\n",
    "    5. Keep punctuation and capitalization\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text_stemmed(text):\n",
    "    \"\"\"\n",
    "    Cleaning with stemming - for comparison with lemmatization\n",
    "    \n",
    "    Steps:\n",
    "    1. All basic cleaning steps\n",
    "    2. Expand contractions\n",
    "    3. Remove stopwords\n",
    "    4. Porter Stemming\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    text = remove_special_characters(text, keep_punctuation=False)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    text = remove_short_words(text, min_length=2)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"âœ“ Cleaning pipelines defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e8d52",
   "metadata": {},
   "source": [
    "## 6. Test Cleaning Functions\n",
    "\n",
    "Let's test our cleaning functions on sample texts to verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebfb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample texts\n",
    "sample_texts = [\n",
    "    \"I don't think @modi is doing great! Check https://example.com #politics #election2019\",\n",
    "    \"This is sooooo goooood!!! Can't believe it!!! ðŸ˜ŠðŸ˜ŠðŸ˜Š\",\n",
    "    \"RT @user: The economy is failing... we're doomed!!!\",\n",
    "    df['clean_text'].iloc[0]  # First actual tweet from data\n",
    "]\n",
    "\n",
    "print(\"Testing Cleaning Pipelines\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Original:     {text[:80]}...\")\n",
    "    print(f\"Basic:        {clean_text_basic(text)[:80]}...\")\n",
    "    print(f\"Full:         {clean_text_full(text)[:80]}...\")\n",
    "    print(f\"Minimal:      {clean_text_minimal(text)[:80]}...\")\n",
    "    print(f\"Stemmed:      {clean_text_stemmed(text)[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec322021",
   "metadata": {},
   "source": [
    "## 7. Apply Cleaning to Dataset\n",
    "\n",
    "Apply all cleaning pipelines to the entire dataset and create multiple cleaned versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Enable progress bar for pandas apply\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "\n",
    "print(\"Applying cleaning pipelines to dataset...\")\n",
    "print(f\"Total records: {len(df)}\\n\")\n",
    "\n",
    "# Apply Basic Cleaning - for Word2Vec, GloVe, FastText\n",
    "print(\"1/4 Applying Basic Cleaning...\")\n",
    "df['text_basic'] = df['clean_text'].progress_apply(clean_text_basic)\n",
    "\n",
    "# Apply Full Cleaning - for TF-IDF, Classical ML\n",
    "print(\"\\n2/4 Applying Full Cleaning (with lemmatization)...\")\n",
    "df['text_full'] = df['clean_text'].progress_apply(clean_text_full)\n",
    "\n",
    "# Apply Minimal Cleaning - for BERT, Transformers\n",
    "print(\"\\n3/4 Applying Minimal Cleaning...\")\n",
    "df['text_minimal'] = df['clean_text'].progress_apply(clean_text_minimal)\n",
    "\n",
    "# Apply Stemmed Cleaning - for comparison\n",
    "print(\"\\n4/4 Applying Stemmed Cleaning...\")\n",
    "df['text_stemmed'] = df['clean_text'].progress_apply(clean_text_stemmed)\n",
    "\n",
    "print(\"\\nâœ“ All cleaning pipelines applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b2172",
   "metadata": {},
   "source": [
    "## 8. Post-Cleaning Analysis\n",
    "\n",
    "Analyze the results of text cleaning to verify quality and compare different pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sample cleaned texts\n",
    "print(\"Sample Comparison of Cleaning Methods\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "sample_indices = df.sample(5, random_state=42).index\n",
    "\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\n--- Record {idx} (Sentiment: {df.loc[idx, 'category']}) ---\")\n",
    "    print(f\"Original:    {str(df.loc[idx, 'clean_text'])[:100]}...\")\n",
    "    print(f\"Basic:       {str(df.loc[idx, 'text_basic'])[:100]}...\")\n",
    "    print(f\"Full:        {str(df.loc[idx, 'text_full'])[:100]}...\")\n",
    "    print(f\"Minimal:     {str(df.loc[idx, 'text_minimal'])[:100]}...\")\n",
    "    print(f\"Stemmed:     {str(df.loc[idx, 'text_stemmed'])[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3512d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for cleaned texts\n",
    "def get_text_stats(text_series, name):\n",
    "    \"\"\"Calculate statistics for a text series.\"\"\"\n",
    "    word_counts = text_series.astype(str).apply(lambda x: len(x.split()) if x else 0)\n",
    "    char_lengths = text_series.astype(str).apply(len)\n",
    "    empty_count = (text_series.astype(str).str.strip() == '').sum()\n",
    "    \n",
    "    return {\n",
    "        'Method': name,\n",
    "        'Avg Words': word_counts.mean(),\n",
    "        'Avg Chars': char_lengths.mean(),\n",
    "        'Empty Texts': empty_count,\n",
    "        'Empty %': (empty_count / len(text_series)) * 100\n",
    "    }\n",
    "\n",
    "# Compare all versions\n",
    "stats = pd.DataFrame([\n",
    "    get_text_stats(df['clean_text'], 'Original'),\n",
    "    get_text_stats(df['text_basic'], 'Basic'),\n",
    "    get_text_stats(df['text_full'], 'Full (Lemmatized)'),\n",
    "    get_text_stats(df['text_minimal'], 'Minimal'),\n",
    "    get_text_stats(df['text_stemmed'], 'Stemmed'),\n",
    "])\n",
    "\n",
    "print(\"\\nText Statistics Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7192109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word count distribution after cleaning\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "cleaning_methods = [\n",
    "    ('text_basic', 'Basic Cleaning'),\n",
    "    ('text_full', 'Full Cleaning (Lemmatized)'),\n",
    "    ('text_minimal', 'Minimal Cleaning'),\n",
    "    ('text_stemmed', 'Stemmed Cleaning'),\n",
    "]\n",
    "\n",
    "for ax, (col, title) in zip(axes.flatten(), cleaning_methods):\n",
    "    word_counts = df[col].astype(str).apply(lambda x: len(x.split()) if x else 0)\n",
    "    ax.hist(word_counts, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Word Count')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{title}\\nMean: {word_counts.mean():.1f}, Median: {word_counts.median():.1f}')\n",
    "    ax.axvline(word_counts.mean(), color='red', linestyle='--', label='Mean')\n",
    "    ax.axvline(word_counts.median(), color='green', linestyle='--', label='Median')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Word Count Distribution After Different Cleaning Methods', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504a615",
   "metadata": {},
   "source": [
    "## 9. Handle Empty Texts\n",
    "\n",
    "Check for and handle any empty texts that resulted from cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty texts in each cleaned column\n",
    "print(\"Empty Text Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in ['text_basic', 'text_full', 'text_minimal', 'text_stemmed']:\n",
    "    empty_mask = df[col].astype(str).str.strip() == ''\n",
    "    empty_count = empty_mask.sum()\n",
    "    print(f\"{col}: {empty_count} empty texts ({empty_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Identify rows that are empty in all cleaning methods\n",
    "all_empty_mask = (\n",
    "    (df['text_basic'].astype(str).str.strip() == '') & \n",
    "    (df['text_full'].astype(str).str.strip() == '') & \n",
    "    (df['text_minimal'].astype(str).str.strip() == '')\n",
    ")\n",
    "print(f\"\\nRows empty in all methods: {all_empty_mask.sum()}\")\n",
    "\n",
    "# Show examples of problematic texts\n",
    "if all_empty_mask.sum() > 0:\n",
    "    print(\"\\nExamples of texts that became empty:\")\n",
    "    print(df.loc[all_empty_mask, 'clean_text'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where minimal cleaning resulted in empty text\n",
    "# (these are likely invalid/corrupted records)\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "\n",
    "# Create mask for non-empty minimal text (most permissive cleaning)\n",
    "valid_mask = df['text_minimal'].astype(str).str.strip() != ''\n",
    "\n",
    "# Filter dataset\n",
    "df_clean = df[valid_mask].copy()\n",
    "print(f\"After removing empty texts: {len(df_clean)}\")\n",
    "print(f\"Removed: {len(df) - len(df_clean)} records\")\n",
    "\n",
    "# Reset index\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "# Verify sentiment distribution is preserved\n",
    "print(\"\\nSentiment Distribution (After Cleaning):\")\n",
    "print(df_clean['category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04c93d",
   "metadata": {},
   "source": [
    "## 10. Vocabulary Analysis\n",
    "\n",
    "Analyze the vocabulary to understand the most common words and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_stats(text_series, name, top_n=20):\n",
    "    \"\"\"\n",
    "    Get vocabulary statistics for a text series.\n",
    "    \"\"\"\n",
    "    all_words = ' '.join(text_series.astype(str)).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Vocabulary Statistics: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total words: {len(all_words):,}\")\n",
    "    print(f\"Unique words: {len(word_freq):,}\")\n",
    "    print(f\"Vocabulary density: {len(word_freq)/len(all_words)*100:.2f}%\")\n",
    "    print(f\"\\nTop {top_n} most common words:\")\n",
    "    \n",
    "    for word, count in word_freq.most_common(top_n):\n",
    "        print(f\"  {word}: {count:,}\")\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# Analyze vocabulary for basic and full cleaned text\n",
    "vocab_basic = get_vocabulary_stats(df_clean['text_basic'], 'Basic Cleaning')\n",
    "vocab_full = get_vocabulary_stats(df_clean['text_full'], 'Full Cleaning (Lemmatized)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f846695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Basic cleaning top words\n",
    "top_basic = dict(vocab_basic.most_common(15))\n",
    "axes[0].barh(list(top_basic.keys())[::-1], list(top_basic.values())[::-1], color='steelblue')\n",
    "axes[0].set_xlabel('Frequency')\n",
    "axes[0].set_title('Top 15 Words (Basic Cleaning)')\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Full cleaning top words\n",
    "top_full = dict(vocab_full.most_common(15))\n",
    "axes[1].barh(list(top_full.keys())[::-1], list(top_full.values())[::-1], color='darkorange')\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Top 15 Words (Full Cleaning - Lemmatized)')\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf33978",
   "metadata": {},
   "source": [
    "## 11. Save Cleaned Data\n",
    "\n",
    "Save the cleaned datasets to the processed data directory for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Prepare final dataframes for different use cases\n",
    "\n",
    "# 1. Main cleaned dataset with all versions\n",
    "df_final = df_clean[['clean_text', 'category', 'text_basic', 'text_full', 'text_minimal', 'text_stemmed']].copy()\n",
    "df_final.columns = ['original_text', 'label', 'text_basic', 'text_lemmatized', 'text_minimal', 'text_stemmed']\n",
    "\n",
    "# Map labels for clarity\n",
    "label_map = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
    "df_final['label_name'] = df_final['label'].map(label_map)\n",
    "\n",
    "# Save main dataset\n",
    "df_final.to_csv('../data/processed/twitter_cleaned.csv', index=False)\n",
    "print(f\"âœ“ Saved main cleaned dataset: ../data/processed/twitter_cleaned.csv\")\n",
    "print(f\"  Shape: {df_final.shape}\")\n",
    "\n",
    "# 2. Save individual cleaned versions for quick loading\n",
    "df_basic = df_final[['text_basic', 'label', 'label_name']].copy()\n",
    "df_basic.to_csv('../data/processed/twitter_basic.csv', index=False)\n",
    "print(f\"âœ“ Saved basic cleaned: ../data/processed/twitter_basic.csv\")\n",
    "\n",
    "df_lemma = df_final[['text_lemmatized', 'label', 'label_name']].copy()\n",
    "df_lemma.to_csv('../data/processed/twitter_lemmatized.csv', index=False)\n",
    "print(f\"âœ“ Saved lemmatized: ../data/processed/twitter_lemmatized.csv\")\n",
    "\n",
    "df_minimal = df_final[['text_minimal', 'label', 'label_name']].copy()\n",
    "df_minimal.to_csv('../data/processed/twitter_minimal.csv', index=False)\n",
    "print(f\"âœ“ Saved minimal cleaned: ../data/processed/twitter_minimal.csv\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All datasets saved successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98281950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataset summary\n",
    "print(\"Final Dataset Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {df_final.shape}\")\n",
    "print(f\"\\nColumns: {df_final.columns.tolist()}\")\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(df_final['label_name'].value_counts())\n",
    "print(f\"\\nSample Records:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93124d",
   "metadata": {},
   "source": [
    "## 12. Summary & Recommendations\n",
    "\n",
    "### Cleaning Methods Summary\n",
    "\n",
    "| Method | Best For | Description |\n",
    "|--------|----------|-------------|\n",
    "| **Basic** | Word2Vec, GloVe, FastText | Removes URLs, mentions, special chars; lowercase |\n",
    "| **Lemmatized** | TF-IDF, Bag of Words, Classical ML | Basic + stopword removal + lemmatization |\n",
    "| **Minimal** | BERT, RoBERTa, Transformers | Only removes URLs; preserves natural text |\n",
    "| **Stemmed** | Comparison experiments | Basic + stemming (more aggressive reduction) |\n",
    "\n",
    "### Recommendations for Model Training\n",
    "\n",
    "1. **For Word Embeddings (Word2Vec, GloVe, FastText)**:\n",
    "   - Use `text_basic` - preserves word forms while removing noise\n",
    "   \n",
    "2. **For TF-IDF / Classical ML (SVM, Naive Bayes, Random Forest)**:\n",
    "   - Use `text_lemmatized` - reduces vocabulary size, removes stopwords\n",
    "   \n",
    "3. **For Transformer Models (BERT, RoBERTa, DistilBERT)**:\n",
    "   - Use `text_minimal` - transformers handle raw text better\n",
    "   \n",
    "4. **For Comparison Studies**:\n",
    "   - Use `text_stemmed` to compare with lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
